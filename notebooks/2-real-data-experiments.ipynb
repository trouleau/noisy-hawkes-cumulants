{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External libs\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "# lib for: ADM4, NPHC, WH\n",
    "from tick.hawkes.inference import HawkesConditionalLaw, HawkesADM4, HawkesCumulantMatching\n",
    "from tick.dataset import fetch_hawkes_bund_data\n",
    "# lib for: Desync-MLE \n",
    "from desync_mhp.lib.inference import HawkesExpKernConditionalMLE\n",
    "\n",
    "# Internal lib\n",
    "import lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define general helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotmat_sidebyside(mats, grid, figsize=(5.5, 1.95), ticks=None, ytitle=None):\n",
    "    \"\"\"Plot matrices side-by-side with the same color scheme\"\"\"\n",
    "    labels = list(mats.keys())\n",
    "    mats = list(mats.values())\n",
    "    n = len(mats)\n",
    "    # Build colormap\n",
    "    vmin = min(map(lambda A: A.min(), mats))\n",
    "    vmax = max(map(lambda A: A.max(), mats))\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = 'plasma'\n",
    "    # Plot matrices\n",
    "    fig, axs = plt.subplots(*grid, figsize=figsize)\n",
    "    axs = np.ravel(axs)\n",
    "    for ax, M, label in zip(axs, mats, labels):\n",
    "        plt.sca(ax)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_aspect(1.0)\n",
    "        dim = len(M)\n",
    "        X = np.tile(np.arange(dim+1)+0.5, (dim+1,1))\n",
    "        Y = X.T\n",
    "        p = plt.pcolormesh(X, Y, M, norm=norm, cmap=cmap)\n",
    "        if ticks:\n",
    "            plt.xticks(ticks)\n",
    "            plt.yticks(ticks)\n",
    "        p.cmap.set_over('white')\n",
    "        p.cmap.set_under('black')\n",
    "        plt.title(label, pad=10, y=ytitle)\n",
    "        # create an axes on the right side of ax. The width of cax will be 5%\n",
    "        # of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        plt.colorbar(p, cax=cax)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the dataset\n",
    "\n",
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = fetch_hawkes_bund_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore inter-event time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_event_time_list = list()\n",
    "for raw_events_r in events:\n",
    "    for raw_events_r_i in raw_events_r:\n",
    "        t_diff = np.diff(np.unique(raw_events_r_i))\n",
    "        assert t_diff.min() > 0\n",
    "        inter_event_time_list.append(t_diff)\n",
    "        \n",
    "plt.hist(np.log10(np.hstack(inter_event_time_list)), bins=20);\n",
    "plt.xlabel('(log) inter-event time')\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset. Use the first 5 days for hyperparameter tuning, use the remaining 15 days for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_events = events[:5]\n",
    "train_events = events[5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Run Experiments Under Random Translation\n",
    "\n",
    "### 2.1. Define helper functions for the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Define a helper function to add random translations to a given event dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_data_noisy(noise_scale, events, seed=None):\n",
    "    noise_sampler = np.random.RandomState(seed)\n",
    "    dim = len(events)\n",
    "    # Add noise to the data\n",
    "    noisy_events = list()\n",
    "    for r, events_r in enumerate(events):\n",
    "        noisy_events.append([])\n",
    "        for i, events_r_i in enumerate(events_r):\n",
    "            noisy_events_r_i = np.sort(events_r_i + noise_sampler.normal(scale=noise_scale, size=events_r_i.shape))\n",
    "            noisy_events[r].append(noisy_events_r_i)\n",
    "    return noisy_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Define helper functions to run each learning method and perform hyperparameter tuning\n",
    "\n",
    "#### 2.2.1. ADM4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best exponential decay providing the largest log-likelihood (i.e. the smallest loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_search_range = np.logspace(3, 3.5, 10)\n",
    "\n",
    "adm4_best_loss = np.inf\n",
    "adm4_best_decay = np.nan\n",
    "for test_decay in decay_search_range:\n",
    "    adm4_dev = HawkesADM4(decay=test_decay, verbose=False)\n",
    "    adm4_dev.fit(dev_events)\n",
    "    loss = adm4_dev.objective(adm4_dev.coeffs)\n",
    "    print(f\"{test_decay:.2e} -> {loss:.2e}\")\n",
    "    if loss < adm4_best_loss:\n",
    "        adm4_best_loss = loss\n",
    "        adm4_best_decay = test_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find best regularization weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_search_range = np.logspace(0, 5, 6)\n",
    "\n",
    "adm4_best_loss = np.inf\n",
    "adm4_best_C = np.nan\n",
    "for test_C in C_search_range:\n",
    "    adm4_dev = HawkesADM4(decay=adm4_best_decay, C=test_C, verbose=False)\n",
    "    adm4_dev.fit(dev_events)\n",
    "    loss = adm4_dev.objective(adm4_dev.coeffs)\n",
    "    print(f\"{test_C:.2e} -> {loss:.2e}\")\n",
    "    if loss < adm4_best_loss:\n",
    "        adm4_best_loss = loss\n",
    "        adm4_best_C = test_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adm4(events, return_extra=False):\n",
    "    adm4 = HawkesADM4(decay=1291.0, C=1e3, verbose=False)\n",
    "    adm4.fit(events)\n",
    "    if return_extra:\n",
    "        return adm4.adjacency.copy(), {'obj': adm4}\n",
    "    return adm4.adjacency.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. WH\n",
    "\n",
    "Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wh(events):\n",
    "    wh = HawkesConditionalLaw(\n",
    "        claw_method=\"log\", delta_lag=0.1, min_lag=5e-4, max_lag=500,\n",
    "        quad_method=\"log\", n_quad=10, min_support=1e-4, max_support=1, n_threads=4)\n",
    "    wh.fit(events)\n",
    "    return wh.get_kernel_norms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. NPHC\n",
    "\n",
    "Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nphc(events, H=4.19e-04):\n",
    "    nphc = HawkesCumulantMatching(integration_support=H, \n",
    "                                  step=1e-2, solver='adam', max_iter=10000, \n",
    "                                  C=1e5, elastic_net_ratio=0.5, penalty='elasticnet', verbose=False)\n",
    "    nphc.fit(events)\n",
    "    return nphc.adjacency.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Desync-MLE\n",
    "\n",
    "Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_desyncmle(events, return_extra=False, verbose=False):    \n",
    "    dim = len(events[0])\n",
    "    # Desync-MLE (Use the same exponential decay and regularization scheme as ADM4)\n",
    "    desyncmle = HawkesExpKernConditionalMLE(\n",
    "        decay=1291.0,\n",
    "        noise_penalty='l2', noise_C=1e5,\n",
    "        hawkes_penalty='l1', hawkes_base_C=1e3, hawkes_adj_C=1e5, \n",
    "        solver='sgd', tol=1e-4, max_iter=1000,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    desyncmle.fit(events, end_time=max([max(map(max, ev)) for ev in events]),\n",
    "                z_start=np.zeros(dim),\n",
    "                theta_start=np.hstack((\n",
    "                    0.01*np.ones(dim),\n",
    "                    np.random.uniform(0.1, 0.2, size=dim**2),\n",
    "                )),\n",
    "                callback=None)\n",
    "    desyncmle_adj = np.reshape(desyncmle.coeffs[2*dim:], (dim, dim))\n",
    "    if return_extra:\n",
    "        return desyncmle_adj, {'z': desyncmle.coeffs[:dim], 'mu': desyncmle.coeffs[dim:2*dim], 'obj': desyncmle}\n",
    "    return desyncmle_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.3. Run experiments for varying noise levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First run each method on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm4_base_adj = run_adm4(train_events)\n",
    "wh_base_adj = run_wh(train_events)\n",
    "nphc_base_adj = run_nphc(train_events)\n",
    "desyncmle_base_adj = run_desyncmle(train_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the excitation matrix learned by each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotmat_sidebyside(\n",
    "    {\n",
    "        '(a) ADM4': adm4_base_adj, \n",
    "        '(b) NPHC': nphc_base_adj, \n",
    "        '(c) WH': wh_base_adj,\n",
    "        '(d) Desync-MLE': desyncmle_base_adj\n",
    "    }, grid=(1, 4), figsize=(6.75, 3.25), ytitle=1.0, ticks=[1, 2, 3, 4])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then run them on each method in randomly translated datasets for varying noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise scale range to iterate over\n",
    "noise_scale_range = np.logspace(np.log10(1e-1 /adm4_best_decay), np.log10(1e4 / adm4_best_decay), 10)\n",
    "# Seed of random noise to iterate over\n",
    "noise_seed_list = np.random.RandomState(703994370).randint(0, 2**32 - 1, size=20)\n",
    "data = list()\n",
    "# For each noise scale\n",
    "for it, noise_scale in enumerate(noise_scale_range):\n",
    "    # Simulate several datasets\n",
    "    for it2, noise_seed in enumerate(noise_seed_list):\n",
    "        print(f\"Iter {it+1}/{len(noise_scale_range)} | {it2+1}/{len(noise_seed_list)} | noise_scale={noise_scale:.2e}\")\n",
    "        # Simulate noisy data\n",
    "        noisy_train_events = generate_real_data_noisy(\n",
    "            noise_scale=noise_scale, events=train_events, seed=noise_seed)\n",
    "        # ADM4\n",
    "        adm4_adj = run_adm4(noisy_train_events)\n",
    "        # WH\n",
    "        wh_adj = run_wh(noisy_train_events)\n",
    "        # NPHC\n",
    "        nphc_adj = run_nphc(noisy_train_events, H=1/1291.0+noise_scale)\n",
    "        # Store estimations\n",
    "        data.append({\n",
    "            'noise_scale': noise_scale,\n",
    "            'noise_seed': noise_seed,\n",
    "            'adm4_adj': adm4_adj,\n",
    "            'wh_adj': wh_adj,\n",
    "            'nphc_adj': nphc_adj,\n",
    "        })\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_queries = [\n",
    "    (\n",
    "        'adm4', \n",
    "        'ADM4'\n",
    "    ),\n",
    "    (\n",
    "        'nphc', \n",
    "        'NPHC'\n",
    "    ),\n",
    "    (\n",
    "        'wh',   \n",
    "        'WH'\n",
    "    )\n",
    "]\n",
    "\n",
    "metric_queries = [\n",
    "    (\n",
    "        'norm',   \n",
    "        'Norm',           \n",
    "        lambda y_test, y_true: np.linalg.norm(y_test.ravel(), ord=2)**2                  \n",
    "    ),\n",
    "    (\n",
    "        'relerr', \n",
    "        'Relative Error', \n",
    "        lambda y_test, y_true: lib.utils.metrics.relerr(y_test, y_true, null_norm='min')\n",
    "    )\n",
    "]\n",
    "\n",
    "base_adj_dict = {\n",
    "    'adm4': adm4_base_adj,\n",
    "    'wh': wh_base_adj,\n",
    "    'nphc': nphc_base_adj,\n",
    "}\n",
    "\n",
    "for method, _ in method_queries:\n",
    "    for suffix, _, func in metric_queries:\n",
    "        df[f'{method}_{suffix}'] = df[f'{method}_adj'].apply(lambda adj: func(y_test=adj, y_true=base_adj_dict[method]))\n",
    "\n",
    "df_plot = df.groupby('noise_scale').agg(['mean', 'std', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for suffix, metric_label, _ in metric_queries:    \n",
    "    print(metric_label, flush=True)\n",
    "    plt.figure(figsize=(3.25, 3.25*0.5*0.85))\n",
    "    plt.grid()\n",
    "    for method, method_label in method_queries:\n",
    "        yseries = df_plot[f'{method}_{suffix}']\n",
    "        y = yseries['mean']\n",
    "        yerr = yseries['std'] / np.sqrt(yseries['count'])\n",
    "        plt.errorbar(df_plot.index, \n",
    "                     y=y, \n",
    "                     yerr=yerr, \n",
    "                     label=method_label, )\n",
    "    \n",
    "    if suffix == 'norm':\n",
    "        plt.yscale('log')\n",
    "    if suffix.startswith('relerr'):\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(f'Noise scale $\\sigma^2$')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}